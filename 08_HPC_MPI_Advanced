* 일정 : 2019/03/19 - 2019/03/22 (4일)
* 장소 : 한국과학기술정보연구원(대전 카이스트)

# MPI 고급



# 21일

* 커뮤니케이터를 쪼개서 만들 수도 있다.
* 커뮤니케이터가 2개일때 자기내들끼리 통신은 어제처럼
* 커뮤니케이터끼리 서로 접속해서 메모리접속할 때? 이건 오늘.
* 오늘할 토플로지

### 

`select=2:ncpus=4:mpiprocs=4:ompthreads=1 -l walltime=04:00:00 -q debug`

* `select=2` 노드2개 할당
* `ncpus=4` 각 노드당 CPU 4개 확보
* `mpiprocs=4` mpi process 4개 할당
* `ompthreads=1` openmp 안쓰니까 1개만
* `-l` 자원요청
* `exit` 다 쓰고 꼭 빠져나오기
```
[sedu14@login04 C]$ qsub -V -I -l select=2:ncpus=4:mpiprocs=4:ompthreads=1 -l walltime=04:00:00 -q debug
qsub: waiting for job 2010096.pbs to start
qsub: job 2010096.pbs ready
```

### 14 토폴로지

좌표형태지만 링타입도 만들 수 있고 그래프타입 등 다양하게 만들 수 있다. 가장 만힝 쓰는게 직교좌표. 그래서 그림을 가져왔다.
* 토폴로지 만들면서 랭크를 재정리
* 좌표형태는 코드가 쉬워질 수있다.
* 주기성을 줘서 반복? 3번 옆에 1번 있다고 가정, 할 수 있다. 
* 없다면 MPIPROCNULL 통신시도 하지 않는다

카티션좌표개

카티시프트 6번의 0방향(위쪽)에 있는 값이 뭐냐? 이웃한 랭크값

MPI CART SHIFT

### 16 
MPI_DIMS_CREATE

포트란 서브루틴 호출 때는 CALL

### 18

직교좌표만들때 쓰는거 CART_CREATE
새로운 커뮤니케이터, 몇차원만들꺼냐?

### 20

2차원, MPI_DIMS에서 정의한걸 사용, 리오더는 TRUE, comm_cart


```fortran
program cart_create
use mpi_f08
implicit none
integer, parameter::ndim=2
integer::myrank,nprocs, dims(ndim)=0, newprocs, newrank
logical::reorder=.true.,periods(ndim)=.false.
type(mpi_comm)::COMM_CART
call MPI_init
call MPI_Comm_rank(MPI_COMM_WORLD, myrank)
CALL MPI_COMM_SIZE(MPI_COMM_WORLD,nprocs)
call MPI_DIMS_CREATE(NPROCS, NDIM,DIMS)
IF(MYRANK==0) PRINT*, 'DIMS=',DIMS
CALL MPI_CART_CREATE(MPI_COMM_WORLD,NDIM,DIMS,PERIODS,REORDER,COMM_CART)
CALL MPI_COMM_SIZE(COMM_CART, NEWPROCS)
CALL MPI_COMM_RANK(COMM_CART, NEWRANK);
PRINT*, 'MYRANK:', MYRANK,'NEWRANK:',NEWRANK
CALL MPI_FINALIZE
end program cart_create
```

```
[sedu14@node8289 to]$ vi 01.f90
[sedu14@node8289 to]$ mpif90 01.f90 -o 01.x
[sedu14@node8289 to]$ mpirun -np 12 ./01.x
 DIMS=           4           3
 MYRANK:           0 NEWRANK:           0
 MYRANK:           2 NEWRANK:           2
 MYRANK:           3 NEWRANK:           3
 MYRANK:           4 NEWRANK:           4
 MYRANK:           5 NEWRANK:           5
 MYRANK:           6 NEWRANK:           6
 MYRANK:           7 NEWRANK:           7
 MYRANK:           1 NEWRANK:           1
 MYRANK:           8 NEWRANK:           8
 MYRANK:           9 NEWRANK:           9
 MYRANK:          10 NEWRANK:          10
 MYRANK:          11 NEWRANK:          11
```

새로만들어진 커뮤니케이터, 

### 26

```


```

### 28

### 33 cart shift

```fortran

PROGRAM CART_SHIFT
use mpi_f08
implicit none
type(mpi_comm)::OLDCOMM, NEWCOMM
integer::ndims=2, DIMSIZE(0:1)
LOGICAL::PERIODS(0:1),REORDER
integer::myrank,nprocs,I,J,RANK
INTEGER::COORDS(0:1)
INTEGER::DIRECTION, DISP,SRC,DEST

call MPI_init
call MPI_Comm_rank(MPI_COMM_WORLD, myrank)
CALL MPI_COMM_SIZE(MPI_COMM_WORLD,nprocs)

OLDCOMM=MPI_COMM_WORLD
DIMSIZE=(/3,2/)
PERIODS=(/.TRUE.,.FALSE./)
REORDER=.TRUE.

CALL MPI_CART_CREATE(OLDCOMM, NDIMS, DIMSIZE,PERIODS, REORDER, NEWCOMM)
DIRECTION=0; DISP=1
CALL MPI_CART_SHIFT(NEWCOMM, DIRECTION, DISP,SRC, DEST)
PRINT*, 'RANK:', MYRANK,'SOURCE:',SRC, 'DESTINATION=',DEST
CALL MPI_FINALIZE
end program CART_SHIFT

```


```
[sedu14@node8289 to]$ mpif90 cart_shift.f90 -o cart_shift.x

[sedu14@node8289 to]$ mpirun -np 6 ./cart_shift.x
 RANK:           0 SOURCE:           4 DESTINATION=           2
 RANK:           1 SOURCE:           5 DESTINATION=           3
 RANK:           2 SOURCE:           0 DESTINATION=           4
 RANK:           3 SOURCE:           1 DESTINATION=           5
 RANK:           4 SOURCE:           2 DESTINATION=           0
 RANK:           5 SOURCE:           3 DESTINATION=           1

```

### 39 neighborhood_allgather

```fortran

PROGRAM neighborhood_allgather
use mpi_f08
implicit none
integer::myrank, procs
type(mpi_comm)::COMM_CART
integer::ndim=2, DIMS(2), COORDS(2), RECVBUF(4)=-10
LOGICAL::PERIODS(2)=.FALSE., REORDER=.FALSE.

call MPI_init
call MPI_Comm_rank(MPI_COMM_WORLD, myrank)
CALL MPI_COMM_SIZE(MPI_COMM_WORLD,procs)

DIMS=(/4,4/)

CALL MPI_CART_CREATE(MPI_COMM_WORLD, NDIM, DIMS, PERIODS,REORDER,COMM_CART)
CALL mpi_CART_COORDS(COMM_CART, MYRANK, NDIM, COORDS)
CALL MPI_NEIGHBORHOOD_ALLGATHER(MYRANK,1,MPI_INTEGER,RECVBUF,1,MPI_INTEGER,COMM_CART)
PRINT*,'MYRANK:', MYRANK,'RECVBUF:',RECVBUF
CALL MPI_FINALIZE
end program neighborhood_allgather


```

### 43 

### 50 win fence 

동기화, 메모리 펜스와 비슷한 개념

mpi foot 비동기함수

winfence 지정하면 다른 프로세서가 접근 가능,

### 62 accumulate

다 있는데 op, win. 연산취하는, 어제 배우는 reduce 연산 그대로 사용 가능.


###

* https://twitter.com/cvpr2019
* https://s2019.siggraph.org/conference/programs-events/computer-animation-festival/electronic-theater/electronic-theater-submissions/
* https://iclr.cc/
* http://iccv2019.thecvf.com/
* https://aideadlin.es/?sub=ML,CV,NLP,RO,SP,DM
* https://brunch.co.kr/@needleworm/61


### 70 get_accumulate

```fortan
PROGRAM get_accumulate
use mpi_f08
implicit none
integer::a,b=0, myrank
integer(kind=MPI_ADDRESS_KIND)::size, disp
type(MPI_Win)::win

CALL MPI_Init
CALL MPI_Comm_rank(MPI_COMM_WORLD, myrank)
if(myrank==0) a=1
if(myrank==1) a=10
if(myrank==2) a=100
print*,'myrank:',myrank,'a=', a
size=storage_size(a)/8
CALL MPI_Win_create(a,size,1,MPI_INFO_NULL,MPI_COMM_WORLD,win)
CALL MPI_Win_fence(0,win)
disp =0
if(myrank==0) then
    CALL
MPI_Get_accumulate(a,1,MPI_Integer,b,1,MPI_Integer,2,disp,1,&MPI_Integer,MPI_Sum,win)
endif

CALL MPI_Win_fence(0,win)
print*,'myrank:',myrank, 'A=', a
print*,'myrank:',myrank, 'A=', b
CALL MPI_Win_free(win)
CALL MPI_FINALIZE
end program get_accumulate

```


```

[sedu14@node8289 to]$ mpif90 get_accumulate.f90 -o get_accumulate.x
get_accumulate.f90:19:69:

     call MPI_Get_accumulate(a,1,MPI_Integer,b,1,MPI_Integer,2,disp,1,&MPI_Integer,MPI_SUM,win)
                                                                     1
Error: Syntax error in argument list at (1)
[sedu14@node8289 to]$

```




```


[sedu14@login03 Codes]$ time mpirun -np 8 ./pi1.x
--------------------------------------------------------------------------
WARNING: No preset parameters were found for the device that Open MPI
detected:

  Local host:            login03
  Device name:           i40iw0
  Device vendor ID:      0x8086
  Device vendor part ID: 14291

Default device parameters will be used, which may result in lower
performance.  You can edit any of the files specified by the
btl_openib_device_param_files MCA parameter to set values for your
device.

NOTE: You can turn off this warning by setting the MCA parameter
      btl_openib_warn_no_device_params_found to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           login03
  Local device:         i40iw0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
ista=250000001, iend =375000000
ista=875000001, iend =1000000000
ista=1, iend =125000000
ista=375000001, iend =500000000
ista=625000001, iend =750000000
ista=750000001, iend =875000000
ista=125000001, iend =250000000
ista=500000001, iend =625000000
numerical pi = 3.141592653589769
analytical pi = 3.141592653589793
Error = 2.442491E-14
[login03:118329] 23 more processes have sent help message help-mpi-btl-openib.txt / no device params found
[login03:118329] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[login03:118329] 15 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port

real    0m2.557s
user    0m13.425s
sys     0m1.367s
[sedu14@login03 Codes]$


```



```
[sedu14@login03 Codes]$ time mpirun -np 12 ./pi1.x
--------------------------------------------------------------------------
WARNING: No preset parameters were found for the device that Open MPI
detected:

  Local host:            login03
  Device name:           i40iw0
  Device vendor ID:      0x8086
  Device vendor part ID: 14291

Default device parameters will be used, which may result in lower
performance.  You can edit any of the files specified by the
btl_openib_device_param_files MCA parameter to set values for your
device.

NOTE: You can turn off this warning by setting the MCA parameter
      btl_openib_warn_no_device_params_found to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           login03
  Local device:         i40iw0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
ista=500000003, iend =583333335
ista=1, iend =83333334
ista=166666669, iend =250000002
ista=666666669, iend =750000001
ista=250000003, iend =333333336
ista=333333337, iend =416666669
ista=750000002, iend =833333334
ista=833333335, iend =916666667
ista=916666668, iend =1000000000
ista=583333336, iend =666666668
ista=83333335, iend =166666668
ista=416666670, iend =500000002
numerical pi = 3.141592653589860
analytical pi = 3.141592653589793
Error = 6.661338E-14
[login03:139380] 35 more processes have sent help message help-mpi-btl-openib.txt / no device params found
[login03:139380] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[login03:139380] 23 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port

real    0m2.080s
user    0m13.668s
sys     0m2.119s


```


```

[sedu14@login03 Codes]$ time mpirun -np 2 ./pi1.x
--------------------------------------------------------------------------
WARNING: No preset parameters were found for the device that Open MPI
detected:

  Local host:            login03
  Device name:           i40iw0
  Device vendor ID:      0x8086
  Device vendor part ID: 14291

Default device parameters will be used, which may result in lower
performance.  You can edit any of the files specified by the
btl_openib_device_param_files MCA parameter to set values for your
device.

NOTE: You can turn off this warning by setting the MCA parameter
      btl_openib_warn_no_device_params_found to 0.
--------------------------------------------------------------------------
[login03:145540] [[15059,0],0] ORTE_ERROR_LOG: Out of resource in file util/show_help.c at line 501
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           login03
  Local device:         i40iw0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
ista=1, iend =500000000
ista=500000001, iend =1000000000
[login03:145540] 4 more processes have sent help message help-mpi-btl-openib.txt / no device params found
[login03:145540] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[login03:145540] 3 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
numerical pi = 3.141592653589901
analytical pi = 3.141592653589793
Error = 1.079137E-13

real    0m7.653s
user    0m13.198s
sys     0m0.308s
```


```c

#include <stdio.h>
#include <mpi.h>
#include <math.h>
#define num_steps 1000000000

int min(int x, int y){

    int v;
    if (x>=y) v=y;
    else v=x;
    return v;
}
void para_range(int n1, int n2, int nprocs, int myrank, int *ista, int *iend){

    int iwork1, iwork2;
    iwork1 = (n2-n1+1)/nprocs;
    iwork2 = (n2-n1+1)%nprocs;
    *ista = myrank*iwork1 + n1 + min(myrank,iwork2);
    *iend = *ista + iwork1 - 1;
    if(iwork2> myrank) *iend = *iend + 1;
}

void main(int argc, char *argv[]){
    double sum, step, x, pi;
    double tsum;
    int i, nprocs, myrank;
    int ista, iend;
    MPI_Status status;
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

    sum=0.0;
    step=1./(double)num_steps;

    para_range(1,num_steps, nprocs, myrank, &ista, &iend);
    printf("ista=%d, iend =%d \n", ista, iend);

    for(i=ista; i<=iend; i++){

        x=(i-0.5)*step;
        sum = sum+4.0/(1.0+x*x);
    }

    MPI_Reduce(&sum, &tsum, 1, MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);

    if(myrank==0){
        pi=step*tsum;
        printf("numerical pi = %.15lf \n", pi);
        printf("analytical pi = %.15f \n", acos(-1.0));
        printf("Error = %E \n", fabs(acos(-1.0)-pi));

    }

    MPI_Finalize();

}


```



```c

#include <stdio.h>
#include <mpi.h>
#define n 11





int min(int x, int y){

    int v;
    if (x>=y) v=y;
    else v=x;
    return v;
}
void para_range(int n1, int n2, int nprocs, int myrank, int *ista, int *iend){


    int iwork1, iwork2;
    iwork1 = (n2-n1+1)/nprocs;
    iwork2 = (n2-n1+1)%nprocs;
    *ista = myrank*iwork1 + n1 + min(myrank,iwork2);
    *iend = *ista + iwork1 - 1;
    if(iwork2> myrank) *iend = *iend + 1;
}

int main(int argc, char *argv[]){
    void para_range(int,int,int,int,int*,int*);
    int min(int, int);
    int i, nprocs, myrank;
    double a[n], b[n];
    int ista, iend, ista2, iend1, inext, iprev;
    MPI_Request isend1, isend2, irecv1, irecv2;
    MPI_Status istatus;
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
    para_range(0, n-1, nprocs, myrank, &ista, &iend);
    ista2 = ista; iend1 = iend;
    if(myrank==0) ista2=1;
    if(myrank==nprocs-1) iend1=n-2;
    inext=myrank+1; iprev=myrank-1;

    if(myrank==nprocs-1) inext=MPI_PROC_NULL;
    if(myrank==0) iprev=MPI_PROC_NULL;
    MPI_Isend(&b[iend], 1, MPI_DOUBLE, inext, 1, MPI_COMM_WORLD, &isend1);
    MPI_Isend(&b[ista], 1, MPI_DOUBLE, iprev, 1, MPI_COMM_WORLD, &isend2);
    MPI_Irecv(&b[ista-1], 1, MPI_DOUBLE, iprev, 1, MPI_COMM_WORLD, &irecv1);
    MPI_Irecv(&b[iend+1], 1, MPI_DOUBLE, inext, 1, MPI_COMM_WORLD, &irecv1);
    MPI_Wait(&isend1, &istatus);
    MPI_Wait(&isend2, &istatus);
    MPI_Wait(&irecv1, &istatus);
    MPI_Wait(&irecv2, &istatus);

    for(i=ista2; i<=iend1; i++) a[i] = b[i-1] + b[i+1];
    MPI_Finalize();

}



```
